# üì¶ Proyecto Sistemas Distribuidos ‚Äì Entrega 2

Este sistema permite recolectar, almacenar, filtrar y analizar eventos de tr√°fico extra√≠dos desde Waze, utilizando tecnolog√≠as distribuidas como **Elasticsearch**, **Apache Pig** y **Hadoop**. A trav√©s de un pipeline completo, los datos se normalizan, se procesan en paralelo y se preparan para la toma de decisiones geogr√°ficas y temporales.

---

## ‚úÖ Requisitos previos

- **Docker** y **Docker Compose** instalados.
- **Node.js v14+** para ejecutar los m√≥dulos JavaScript.
- Permisos de superusuario (`sudo`) si el entorno lo requiere.

---

## üöÄ Despliegue con Docker Compose

### 1. Detener contenedores previos (opcional)
```bash
sudo docker compose down
```

### 2. Construir y levantar los servicios
```bash
sudo docker compose build --no-cache
sudo docker compose up
```

Esto iniciar√° los siguientes componentes:

- **Elasticsearch** (https://localhost:9200)  
  - Usuario: `elastic`  
  - Contrase√±a: `changeme`

- **Scraper**
  - Indexa el archivo inicial `traffic_data.csv` (usado como dataset base).
  - Inicia el scraping en tiempo real de eventos desde Waze.

- **Servidor de consultas internas** (http://localhost:8080)  
  - Permite ejecutar b√∫squedas simples sobre los eventos almacenados.

---

## üß† Filtrado y an√°lisis distribuido (Apache Pig)

### 1. Asegurarse de que Docker est√© corriendo y que `traffic_data.csv` ya est√© indexado en Elasticsearch.

### 2. Ejecutar el an√°lisis
```bash
cd hadoop/
./run_analysis.sh
```

Este script ejecuta el perfil `hadoop-analytics` de Docker Compose y realiza:

1. Extracci√≥n de eventos desde Elasticsearch (`fetch_from_elastic.sh`)
2. Filtrado y normalizaci√≥n (`filter_data.pig`)
3. Procesamiento anal√≠tico (`process_events.pig`)

### 3. Visualizar los resultados

Una vez finalizado el an√°lisis, espera a que el sistema muestre el mensaje:

> `An√°lisis completado, ejecuta ./hadoop/display_results.sh para ver los resultados.`

Luego, ejecuta el siguiente comando para visualizar los resultados procesados:

```bash
./display_results.sh
```

Este script mostrar√° en consola los archivos de salida generados por Apache Pig, los cuales contienen las consultas anal√≠ticas realizadas, como el conteo de eventos por tipo, distribuci√≥n por comuna y evoluci√≥n temporal. Estas salidas representan la fase final del pipeline de an√°lisis.

---

## üîç Scraper ‚Äì Recolecci√≥n de eventos

Ubicaci√≥n: `Proyecto_SD/scrapper/`

### Instalaci√≥n y ejecuci√≥n
```bash
cd scrapper
npm install
node index.js
```

El scraper extrae datos desde Waze usando Puppeteer, proces√°ndolos y envi√°ndolos directamente a Elasticsearch. Incluye mejoras como extracci√≥n de ID de evento, hora reportada y nombre de usuario.

---

## üóÑÔ∏è Servidor de consultas internas (Elasticsearch)

Ubicaci√≥n: `Proyecto_SD/Elastic/`

Este m√≥dulo permite realizar consultas b√°sicas sobre los datos almacenados.

### Instalaci√≥n y ejecuci√≥n
```bash
cd Elastic
npm install
node server.js
```

### Ejemplos de consulta con `curl`
```bash
curl 'http://localhost:8080/consultas'
curl 'http://localhost:8080/consultas?alerttype=alert'
```

---

## üß™ Generador de tr√°fico aleatorio (Entrega 1)

Ubicaci√≥n: `Proyecto_SD/generator/`

Simula peticiones de consulta al sistema usando distintas distribuciones.

### Uso
```bash
cd generator
npm install
node randomGenerator.js <n_consultas> <distribucion> <p1> <p2> <p3>
```

Ejemplo:
```bash
node randomGenerator.js 1000 pareto 1 1 0
```

- `p1`: probabilidad de filtro por comuna  
- `p2`: probabilidad de filtro por tipo de alerta  
- `p3`: probabilidad de filtro por tipo de incidente  

> üîÅ La rama `main` usa pol√≠tica de cacheo LRU (`allkeys-lru`).  
> üîÄ La rama `random_metricas` usa pol√≠tica aleatoria (`allkeys-random`).
